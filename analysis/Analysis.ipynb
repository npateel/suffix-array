{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1e0f36",
   "metadata": {},
   "source": [
    "# Suffix Array Benchmarking and Analysis\n",
    "## Author: Nikhil Pateel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8998fe",
   "metadata": {},
   "source": [
    "### `buildsa`\n",
    "\n",
    "My implementaiton of `buildsa` uses `cereal` and `sdsl`'s `csa_wt` class to build the suffix array table. The way that the prefix table was built was through using an $$O(kn)$$ algorithm that looped through each entry in the suffix array and got the first and last indices of each prefix of length $k$.\n",
    "\n",
    "Below, we'll perform benchmarking based on the time it takes in seconds to build the suffix array with no preftable and a preftable of $k=8$. We use this number because it is sufficiently useful for prefix tables and in the middle of our prefix table benchmarking.\n",
    "\n",
    "We find that, overall, there is a very heavy cost of computing a prefix table (2s for a ~100k NT sequence and approx 20s for E.Coli), but the overall value of the k does not matter too much in the index build time. (Most of the slowdown happens in actually building the prefix table). \n",
    "\n",
    "Building the suffix array takes less than a second for any NTs less than 100k, but then started to take more and more time. The Size increased in proportion to the sequence length.\n",
    "\n",
    "\n",
    "Here's a table of the results for fixed $k$ (preftab = 1) (but they're all similar except for size)\n",
    "\n",
    "| Nucleotide Size | SA build time | Size |\n",
    "| --- | --- | --- |\n",
    "| 700 | 0.047s | 4 kB |\n",
    "| 7k  | 0.040s | 13 kB |\n",
    "| 70k | 0.040s | 156 kB|\n",
    "| 700k| 0.21s  | 1.4 MB|\n",
    "| 7M  | 2.1s   | 13.7 MB|\n",
    "\n",
    "\n",
    "When $k$ changes, the prefix table time increases slightly, but the size of the serialization greatly increases. Here\n",
    "are results at 7M nucleotides\n",
    "\n",
    "| k| Prefix Table Build Time | Size |\n",
    "| --- | --- | --- |\n",
    "| -1| 0 | 13.7MB|\n",
    "| 1 | 55.85s | 13.7MB|\n",
    "| 2 | 55.55s | 13.7MB|\n",
    "| 4 | 55.55s | 13.7MB|\n",
    "| 8 | 56.61s | 23.6MB|\n",
    "| 16| 61.90s | 220MB |\n",
    "| 32 | 62.90s | 360MB|\n",
    "\n",
    "So we can see that for lower values of $k$ (less than 8), the size doesn't get prohibitive, but more than that can cause some serious problems with compression.\n",
    "\n",
    "With this scaling, and no prefix table, I expect that we'd be able to go up to a sequence length of 16 GNTs (16 billion) seeing that the overall overhead of the SA is 2 times the number of nucleotides in bytes (there probably are some issues with compression causing such bloat).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6db64d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "sizes = [10, 100, 1000, 10000]\n",
    "ks = [None, 1, 2, 4, 8, 16, 32]\n",
    "for size in sizes:\n",
    "    for k in ks:\n",
    "        args = ['../bin/buildsa']\n",
    "        if k is not None:\n",
    "            args.append('--preftab')\n",
    "            args.append(str(k))\n",
    "        ref_file = '../' + os.path.join('data', 'samples', 'ecoli-' + str(size) + '.fa')\n",
    "        out_file = '../' + os.path.join('cache', 'ecoli-' + str(size) + '-' + str(k or 'nopref') + '.sa')\n",
    "        args.append(ref_file)\n",
    "        args.append(out_file)\n",
    "        args.append('-b')\n",
    "        args.append('benchmarking.csv')\n",
    "        print(args)\n",
    "        subprocess.run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa38f32",
   "metadata": {},
   "source": [
    "### `querysa`\n",
    "\n",
    "I found `querysa` much harder to implement in general. I'd say that I spent at least twice as long on this project. I found it especially hard to get the prefix table working like I wanted to, and ran into a lot of issues specifically with random edge cases in the prefix table. A very big source of annoyance was that the human genome dataset has a lot of features and noise (lowercase vs uppercase). While my code works for the reference genome and other genomes that I've written myself, it can't seem to work for the human genome overall. There might be some latent bugs, or input issues that are causing this problem.\n",
    "\n",
    "Nevertheless, here are the results I was able to scrap together.\n",
    "\n",
    "When we control for everything besides sequence length, we get the following: (naive k=4, 64 long queries)\n",
    "\n",
    "| Nucleotide Size | Average Query time (ns) |\n",
    "| --- | --- | \n",
    "| 700 | 2413| \n",
    "| 7k  | 17172| \n",
    "| 70k | 62661 | \n",
    "| 700k| 108024  |\n",
    "\n",
    "We see that the runtime is approximately logn with respect to sequence length.\n",
    "\n",
    "When query size changes... (with 70k NTs)\n",
    "\n",
    "\n",
    "| Query Size | Average Query time (ns) |\n",
    "| --- | --- | \n",
    "| 32 | 61863|\n",
    "| 64  | 62661| \n",
    "| 128 | 63196.2 | \n",
    "| 256 | 62021.9  |\n",
    "\n",
    "That is to say that while the expected runtime to increase linearly, we see that the runtimes are largely constant. It's possible that there are large constant factors making this scale better than the expected time complexity.\n",
    "\n",
    "Next, we visit the impact of having a prefix table (assuming 70k NTs, 64 long queries) and query method \n",
    "| k| Average Query time (ns) (naive) | Average Query time (ns) (simpaccel) |\n",
    "| -1| 96173.9| 122531|\n",
    "| 1 | 40020.9|114547|\n",
    "| 2 | 31798.9|118347|\n",
    "| 4 | 17172.4 |103008|\n",
    "| 8 | 645.452 |99520.5|\n",
    "| 16 | 118.22|126894|\n",
    "| 32 | 120.99|126896|\n",
    "\n",
    "We  see that there are diminishing returns as k gets larger and larger. It seems like $k=4$ is a \"sweet spot\". Furthermore, it looks like there's a performance penalty with `simpaccel`, this likely has to do with an intentional slowdown in the lcp compare method (for some reason using `start = smallest` doesn't work in line 169 of `querysa.cpp`, smallest becomes way too big for no reason, but only on lcp with prefix tables).\n",
    "\n",
    "All in all, I believe that having prefix tables are useful, but might be expensive in pre-computation time. It really depends on if an index will be used a significant amount or not. Regardless, since $k$ has little effect on the time complexity, I would suggest a value of $k=4$ or 8 to balance the space complexity with the performance gain on query times.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate queries\n",
    "import random\n",
    "queries = [32, 64, 128, 256]\n",
    "for query in queries:\n",
    "    queryfile = '../' + os.path.join('data', 'queries', str(query)+'.fa')\n",
    "    with open(queryfile, 'w') as f:\n",
    "        for i in range(1000):\n",
    "            f.write('> ' + str(i) + '\\n')\n",
    "            f.write(''.join(random.choices('ACTG', k=query)) + '\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba47914",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate results\n",
    "algos = ['naive', 'simpaccel']\n",
    "sizes = [10, 100, 1000, 10000]\n",
    "ks = [None,1, 2,  4, 8, 16, 32]\n",
    "for size in sizes:\n",
    "    for k in ks:\n",
    "        index = '../' + os.path.join('cache', 'human-' + str(size) + '-' + str(k or 'nopref') + '.sa')\n",
    "        for algo in algos:\n",
    "            for query in queries:\n",
    "                args = ['../bin/querysa']\n",
    "                args.append(index)\n",
    "                queryfile = '../' + os.path.join('data', 'queries', str(query) + '.fa')\n",
    "                args.append(queryfile)\n",
    "                args.append(algo)\n",
    "                args.append('output')\n",
    "                args.append('-b')\n",
    "                args.append('querybenchmark.csv')\n",
    "                print(args)\n",
    "                subprocess.run(args) \n",
    "                \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de16fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f353fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python385jvsc74a57bd0ff42d6d514eb9fa17d59455b8b29a7c58d612ad14207e1c584f96921da866f99"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
